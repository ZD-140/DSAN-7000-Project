{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"cleaned_freakshow_training.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Date</th>\n",
       "      <th>URL</th>\n",
       "      <th>Content</th>\n",
       "      <th>content_length</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>label_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Music</td>\n",
       "      <td>2016-09-22</td>\n",
       "      <td>http://www.usmagazine.com/entertainment/news/t...</td>\n",
       "      <td>By clicking Sign In, you agree to our Terms an...</td>\n",
       "      <td>2509</td>\n",
       "      <td>clicking sign agree term condition read privac...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Politics</td>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>http://www.heraldscotland.com/opinion/14759994...</td>\n",
       "      <td>\\n  IF you are watching the Corbyn-Smith show ...</td>\n",
       "      <td>6499</td>\n",
       "      <td>watching corbynsmith show pondering whether sc...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mass Media</td>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>http://www.notebookreview.com/feature/nbr-flas...</td>\n",
       "      <td>Connect with more active buying teams and shap...</td>\n",
       "      <td>3138</td>\n",
       "      <td>connect active buying team shape decision make...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sideshow</td>\n",
       "      <td>2016-09-25</td>\n",
       "      <td>https://www.loc.gov/item/fsa1997018934/PP/</td>\n",
       "      <td>Top of page \\n\\n\\n    Back to Search Results\\n...</td>\n",
       "      <td>3030</td>\n",
       "      <td>top page back search result content library co...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Music</td>\n",
       "      <td>2016-09-26</td>\n",
       "      <td>http://musicfeeds.com.au/news/parquet-courts-a...</td>\n",
       "      <td>\\n\\tBy\\n\\t\\t\\t\\n\\t\\t\\tMike Hohnen\\t\\t\\n UPDATE...</td>\n",
       "      <td>2188</td>\n",
       "      <td>mike hohnen update parquet court announced mel...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category        Date                                                URL  \\\n",
       "0       Music  2016-09-22  http://www.usmagazine.com/entertainment/news/t...   \n",
       "1    Politics  2016-09-24  http://www.heraldscotland.com/opinion/14759994...   \n",
       "2  Mass Media  2016-09-24  http://www.notebookreview.com/feature/nbr-flas...   \n",
       "3    Sideshow  2016-09-25         https://www.loc.gov/item/fsa1997018934/PP/   \n",
       "4       Music  2016-09-26  http://musicfeeds.com.au/news/parquet-courts-a...   \n",
       "\n",
       "                                             Content  content_length  \\\n",
       "0  By clicking Sign In, you agree to our Terms an...            2509   \n",
       "1  \\n  IF you are watching the Corbyn-Smith show ...            6499   \n",
       "2  Connect with more active buying teams and shap...            3138   \n",
       "3  Top of page \\n\\n\\n    Back to Search Results\\n...            3030   \n",
       "4  \\n\\tBy\\n\\t\\t\\t\\n\\t\\t\\tMike Hohnen\\t\\t\\n UPDATE...            2188   \n",
       "\n",
       "                                     cleaned_content  label_encoded  \n",
       "0  clicking sign agree term condition read privac...             13  \n",
       "1  watching corbynsmith show pondering whether sc...             14  \n",
       "2  connect active buying team shape decision make...             12  \n",
       "3  top page back search result content library co...             15  \n",
       "4  mike hohnen update parquet court announced mel...             13  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 908 entries, 0 to 907\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Category         908 non-null    object\n",
      " 1   Date             853 non-null    object\n",
      " 2   URL              908 non-null    object\n",
      " 3   Content          856 non-null    object\n",
      " 4   content_length   908 non-null    int64 \n",
      " 5   cleaned_content  856 non-null    object\n",
      " 6   label_encoded    908 non-null    int64 \n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 49.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'cleaned_content' miss value：52\n",
      "'label_encoded' miss value：0\n"
     ]
    }
   ],
   "source": [
    "# check cleaned_content ,label_encoded miss value\n",
    "missing_cleaned_content = data['cleaned_content'].isnull().sum()\n",
    "missing_label_encoded = data['label_encoded'].isnull().sum()\n",
    "\n",
    "print(f\"'cleaned_content' miss value：{missing_cleaned_content}\")\n",
    "print(f\"'label_encoded' miss value：{missing_label_encoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856\n"
     ]
    }
   ],
   "source": [
    "data = data.dropna(subset=['cleaned_content']).reset_index(drop=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    8.560000e+02\n",
      "mean     5.131155e+03\n",
      "std      5.860295e+04\n",
      "min      5.000000e+00\n",
      "25%      1.054250e+03\n",
      "50%      2.155500e+03\n",
      "75%      3.513500e+03\n",
      "max      1.708178e+06\n",
      "Name: content_length, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAIhCAYAAABE54vcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABMxklEQVR4nO3de1hVZf7//9eWMwQoqGwZUdFQU8wDlqcKHcCzlk5R2UGL5mNjWZR8nBw/U9gUmiU6o6k1OWA5HjpIx8nETMvRJkUttTKnPBZEGgEqgsL9+8Mf+9sWUNYW2ajPx3Wt63Lf673Wute+3S5frLVvbMYYIwAAAABArTVydwcAAAAA4GJDkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACgDqSmZkpm83mWHx9fWW32zVgwABNnz5d+fn5VbZJTU2VzWazdJzjx48rNTVV69ats7Rddcdq06aNhg8fbmk/57J06VLNmTOn2nU2m02pqal1ery69uGHH6pnz54KCAiQzWbTm2++WaWmf//+TmNd01KX55qWllZtX2pis9n04IMP1tnx69r8+fOVmZlZpX3dunWy2Wx6/fXX679TAGCBp7s7AACXmoyMDHXs2FEnT55Ufn6+NmzYoGeeeUbPPfecVqxYofj4eEftfffdp8GDB1va//HjxzVt2jRJp/9DX1uuHMsVS5cu1c6dO5WcnFxl3aZNm9SyZcsL3gdXGWOUmJio9u3b6+2331ZAQIA6dOhQpW7+/PkqKipyvH7vvff01FNPOca+Ul2ea1pamm6++WbddNNNdbZPd5o/f76aNm2qcePGubsrAOASghQA1LHo6Gj17NnT8fp3v/udHnnkEV133XUaPXq09uzZo7CwMEmn/6N9oYPF8ePH5e/vXy/HOpfevXu79fjn8sMPP+jnn3/WqFGjFBcXV2Ndp06dnF5//fXXkqqOPQDg0sWjfQBQD1q1aqVZs2apuLhYL7zwgqO9usft1q5dq/79+ys0NFR+fn5q1aqVfve73+n48ePat2+fmjVrJkmaNm2a4xGyyp/qV+5v69atuvnmm9WkSRO1a9euxmNVysrK0tVXXy1fX1+1bdtWf/vb35zWVz62uG/fPqf2ysewKh8z7N+/v9577z3t37/f6RG3StU97rZz507deOONatKkiXx9fdWtWzctXry42uMsW7ZMU6dOVXh4uIKCghQfH6/du3fX/Mb/yoYNGxQXF6fAwED5+/urb9++eu+99xzrU1NTHUHzj3/8o2w2m9q0aVOrfddkxYoV6tOnjwICAnTFFVdo0KBB2rZtm1OfvLy8lJKS4rRd5fu9aNEiSafft2PHjmnx4sWO99TK3cialJWV6amnnlLHjh3l4+OjZs2a6Z577tFPP/3kVFf5COiqVavUo0cP+fn5qWPHjvrHP/5RZZ8bNmxQnz595Ovrq9/85jf685//rJdeesnp70+bNm20a9curV+/3nE+Z77XJ0+ePOdYb9u2TcOHD1fz5s3l4+Oj8PBwDRs2TIcOHTrv9wYAzoUgBQD1ZOjQofLw8NDHH39cY82+ffs0bNgweXt76x//+IdWrVqlGTNmKCAgQGVlZWrRooVWrVolSUpKStKmTZu0adMm/fnPf3baz+jRo3XllVfqtdde08KFC8/ar+3btys5OVmPPPKIsrKy1LdvXz388MN67rnnLJ/j/Pnz1a9fP9ntdkffNm3aVGP97t271bdvX+3atUt/+9vftHLlSnXq1Enjxo3TzJkzq9T/6U9/0v79+/XSSy/pxRdf1J49ezRixAiVl5eftV/r16/Xb3/7WxUWFmrRokVatmyZAgMDNWLECK1YsULS6UcfV65cKUmaOHGiNm3apKysLMvvQaW0tDTdfvvt6tSpk1599VW98sorKi4u1vXXX68vv/xSknTdddfpqaee0qxZs/T2229Lknbt2qUHHnhAd955p5KSkiSdfiTSz89PQ4cOdbyn8+fPd7lvklRRUaEbb7xRM2bM0JgxY/Tee+9pxowZys7OVv/+/VVSUuJU//nnn2vSpEl65JFH9NZbb+nqq69WUlKS09/nL774QgkJCTp+/LgWL16shQsXauvWrXr66aed9pWVlaW2bduqe/fujvM5870+11gfO3ZMCQkJ+vHHH/X8888rOztbc+bMUatWrVRcXHxe7w0A1IoBANSJjIwMI8ls3ry5xpqwsDBz1VVXOV4/8cQT5tf/FL/++utGktm+fXuN+/jpp5+MJPPEE09UWVe5v8cff7zGdb/WunVrY7PZqhwvISHBBAUFmWPHjjmd2969e53qPvroIyPJfPTRR462YcOGmdatW1fb9zP7fdtttxkfHx9z4MABp7ohQ4YYf39/88svvzgdZ+jQoU51r776qpFkNm3aVO3xKvXu3ds0b97cFBcXO9pOnTploqOjTcuWLU1FRYUxxpi9e/caSebZZ5896/7OdObYHzhwwHh6epqJEyc61RUXFxu73W4SExMdbRUVFWbo0KGmcePGZufOnaZTp06mY8eO5ujRo07bBgQEmLFjx9a6T5LMAw88UOP6ZcuWGUnmjTfecGrfvHmzkWTmz5/vaGvdurXx9fU1+/fvd7SVlJSYkJAQM378eEfbLbfcYgICAsxPP/3kaCsvLzedOnWq8venc+fOJjY2tkq/ajvWW7ZsMZLMm2++efY3AgAuEO5IAUA9MsacdX23bt3k7e2t//mf/9HixYv13XffuXSc3/3ud7Wu7dy5s7p27erUNmbMGBUVFWnr1q0uHb+21q5dq7i4OEVERDi1jxs3TsePH69yN2vkyJFOr6+++mpJ0v79+2s8xrFjx/Sf//xHN998s6644gpHu4eHh+666y4dOnSo1o8H1tYHH3ygU6dO6e6779apU6cci6+vr2JjY51mXLTZbHr55ZcVGBionj17au/evXr11VcVEBBQp30607vvvqvGjRtrxIgRTn3s1q2b7HZ7lVkhu3XrplatWjle+/r6qn379k7vfeWdv6ZNmzraGjVqpMTERMv9O9dYX3nllWrSpIn++Mc/auHChY67fABQXwhSAFBPjh07piNHjig8PLzGmnbt2mnNmjVq3ry5HnjgAbVr107t2rXTX//6V0vHatGiRa1r7XZ7jW1HjhyxdFyrjhw5Um1fK9+jM48fGhrq9NrHx0eSqjyG9msFBQUyxlg6zvn68ccfJUnXXHONvLy8nJYVK1bo8OHDTvWhoaEaOXKkTpw4ocGDB6tLly512p+a+vjLL7/I29u7Sh/z8vKq7eOZfHx8nN77I0eOOCZS+bXq2s7lXGMdHBys9evXq1u3bvrTn/6kzp07Kzw8XE888YROnjxp+XgAYBWz9gFAPXnvvfdUXl5+zkkCrr/+el1//fUqLy/Xli1bNHfuXCUnJyssLEy33XZbrY5l5XdT5eXl1dhW+Z9ZX19fSVJpaalT3Zn/2bYqNDRUubm5Vdp/+OEHSXK6s+GqJk2aqFGjRhf8OL9Wub/XX39drVu3Pmd9dna2FixYoGuvvVZZWVl64403LN1VdLWPoaGhju/cnSkwMNDyPkNDQx0h8teq+ztWF7p06aLly5fLGKMvvvhCmZmZevLJJ+Xn56fHHnvsghwTACpxRwoA6sGBAweUkpKi4OBgjR8/vlbbeHh4qFevXnr++eclyfGYXW3uwlixa9cuff75505tS5cuVWBgoHr06CFJjhnVvvjiC6e6ygkSfu3MuxRnExcXp7Vr1zoCTaWXX35Z/v7+dTJdekBAgHr16qWVK1c69auiokJLlixRy5Yt1b59+/M+zq8NGjRInp6e+vbbb9WzZ89ql0q5ubm68847FRsbq40bN2rkyJFKSkrS3r17nfZp5X2tjeHDh+vIkSMqLy+vtn/V/f6sc4mNjdXatWudAnZFRYVee+21KrV1eT42m01du3bV7Nmz1bhx4wv+SCoASNyRAoA6t3PnTsf3TfLz8/XJJ58oIyNDHh4eysrKckxfXp2FCxdq7dq1GjZsmFq1aqUTJ044ppiu/EW+gYGBat26td566y3FxcUpJCRETZs2dXmq7vDwcI0cOVKpqalq0aKFlixZouzsbD3zzDPy9/eXdPoRtQ4dOiglJUWnTp1SkyZNlJWVpQ0bNlTZX5cuXbRy5UotWLBAMTExatSoUY2/W+mJJ57Qu+++qwEDBujxxx9XSEiI/vnPf+q9997TzJkzFRwc7NI5nWn69OlKSEjQgAEDlJKSIm9vb82fP187d+7UsmXLLN3Bq402bdroySef1NSpU/Xdd99p8ODBatKkiX788Ud99tlnCggI0LRp01ReXq7bb79dNptNS5culYeHhzIzM9WtWzfdeuut2rBhg7y9vSWdfl/XrVund955Ry1atFBgYOA5w863336r119/vUp7p06ddNttt+mf//ynhg4dqocffljXXnutvLy8dOjQIX300Ue68cYbNWrUKEvnPXXqVL3zzjuKi4vT1KlT5efnp4ULF+rYsWOSTn9fqlLl3aQVK1aobdu28vX1tfRI47vvvqv58+frpptuUtu2bWWM0cqVK/XLL78oISHBUr8BwCXunesCAC4dlTO3VS7e3t6mefPmJjY21qSlpZn8/Pwq25w5k96mTZvMqFGjTOvWrY2Pj48JDQ01sbGx5u2333babs2aNaZ79+7Gx8fHSHLM5la5v1/PmlbTsYw5PRvbsGHDzOuvv246d+5svL29TZs2bUx6enqV7b/55hszcOBAExQUZJo1a2YmTpxo3nvvvSqz9v3888/m5ptvNo0bNzY2m83pmKpmtsEdO3aYESNGmODgYOPt7W26du1qMjIynGoqZ3J77bXXnNorZ9k7s746n3zyifntb39rAgICjJ+fn+ndu7d55513qt3f+c7aV+nNN980AwYMMEFBQcbHx8e0bt3a3HzzzWbNmjXGGGOmTp1qGjVqZD788EOn7TZu3Gg8PT3Nww8/7Gjbvn276devn/H39zeSqp3x7td+/XfxzKVyDE6ePGmee+4507VrV+Pr62uuuOIK07FjRzN+/HizZ88ex74q/56cKTY2tko/PvnkE9OrVy/j4+Nj7Ha7+d///V/zzDPPGEmOWRiNMWbfvn1m4MCBJjAw0EhyzPRY27H++uuvze23327atWtn/Pz8THBwsLn22mtNZmbmWd8XAKgrNmPOMYUUAADAeRg4cKD27dunb775xt1dAYA6w6N9AACgzjz66KPq3r27IiIi9PPPP+uf//ynsrOztWjRInd3DQDqFEEKAADUmfLycj3++OPKy8uTzWZTp06d9Morr+jOO+90d9cAoE7xaB8AAAAAWMT05wAAAABgEUEKAAAAACwiSAEAAACARUw2odO/df2HH35QYGBgnf9SRgAAAAAXD2OMiouLFR4e7vSLxM9EkJL0ww8/KCIiwt3dAAAAANBAHDx4UC1btqxxPUFKUmBgoKTTb1ZQUJCbewMAAADAXYqKihQREeHICDUhSEmOx/mCgoIIUgAAAADO+ZUfJpsAAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGCRp7s7gKoOHDigw4cPW96uadOmatWq1QXoEQAAAIBfI0g1MAcOHFDHq65SyfHjlrf18/fX1199RZgCAAAALjCCVANz+PBhlRw/rsSnFqh5ZFStt8vfu0ev/t8fdPjwYYIUAAAAcIERpBqo5pFR+s1VXd3dDQAAAADVYLIJAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCK3BqlTp07p//7v/xQZGSk/Pz+1bdtWTz75pCoqKhw1xhilpqYqPDxcfn5+6t+/v3bt2uW0n9LSUk2cOFFNmzZVQECARo4cqUOHDtX36QAAAAC4TLg1SD3zzDNauHCh5s2bp6+++kozZ87Us88+q7lz5zpqZs6cqfT0dM2bN0+bN2+W3W5XQkKCiouLHTXJycnKysrS8uXLtWHDBh09elTDhw9XeXm5O04LAAAAwCXO050H37Rpk2688UYNGzZMktSmTRstW7ZMW7ZskXT6btScOXM0depUjR49WpK0ePFihYWFaenSpRo/frwKCwu1aNEivfLKK4qPj5ckLVmyRBEREVqzZo0GDRrknpMDAAAAcMly6x2p6667Th9++KG++eYbSdLnn3+uDRs2aOjQoZKkvXv3Ki8vTwMHDnRs4+Pjo9jYWG3cuFGSlJOTo5MnTzrVhIeHKzo62lFzptLSUhUVFTktAAAAAFBbbr0j9cc//lGFhYXq2LGjPDw8VF5erqefflq33367JCkvL0+SFBYW5rRdWFiY9u/f76jx9vZWkyZNqtRUbn+m6dOna9q0aXV9OgAAAAAuE269I7VixQotWbJES5cu1datW7V48WI999xzWrx4sVOdzWZzem2MqdJ2prPVTJkyRYWFhY7l4MGD53ciAAAAAC4rbr0j9b//+7967LHHdNttt0mSunTpov3792v69OkaO3as7Ha7pNN3nVq0aOHYLj8/33GXym63q6ysTAUFBU53pfLz89W3b99qj+vj4yMfH58LdVoAAAAALnFuvSN1/PhxNWrk3AUPDw/H9OeRkZGy2+3Kzs52rC8rK9P69esdISkmJkZeXl5ONbm5udq5c2eNQQoAAAAAzodb70iNGDFCTz/9tFq1aqXOnTtr27ZtSk9P17333ivp9CN9ycnJSktLU1RUlKKiopSWliZ/f3+NGTNGkhQcHKykpCRNmjRJoaGhCgkJUUpKirp06eKYxQ8AAAAA6pJbg9TcuXP15z//WRMmTFB+fr7Cw8M1fvx4Pf74446ayZMnq6SkRBMmTFBBQYF69eql1atXKzAw0FEze/ZseXp6KjExUSUlJYqLi1NmZqY8PDzccVoAAAAALnE2Y4xxdyfcraioSMHBwSosLFRQUJBb+7J161bFxMTowX+u0W+u6lrr7b7/6nPNuyNeOTk56tGjxwXsIQAAAHDpqm02cOt3pAAAAADgYkSQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABY5NYg1aZNG9lstirLAw88IEkyxig1NVXh4eHy8/NT//79tWvXLqd9lJaWauLEiWratKkCAgI0cuRIHTp0yB2nAwAAAOAy4dYgtXnzZuXm5jqW7OxsSdItt9wiSZo5c6bS09M1b948bd68WXa7XQkJCSouLnbsIzk5WVlZWVq+fLk2bNigo0ePavjw4SovL3fLOQEAAAC49Lk1SDVr1kx2u92xvPvuu2rXrp1iY2NljNGcOXM0depUjR49WtHR0Vq8eLGOHz+upUuXSpIKCwu1aNEizZo1S/Hx8erevbuWLFmiHTt2aM2aNe48NQAAAACXsAbzHamysjItWbJE9957r2w2m/bu3au8vDwNHDjQUePj46PY2Fht3LhRkpSTk6OTJ0861YSHhys6OtpRU53S0lIVFRU5LQAAAABQWw0mSL355pv65ZdfNG7cOElSXl6eJCksLMypLiwszLEuLy9P3t7eatKkSY011Zk+fbqCg4MdS0RERB2eCQAAAIBLXYMJUosWLdKQIUMUHh7u1G6z2ZxeG2OqtJ3pXDVTpkxRYWGhYzl48KDrHQcAAABw2WkQQWr//v1as2aN7rvvPkeb3W6XpCp3lvLz8x13qex2u8rKylRQUFBjTXV8fHwUFBTktAAAAABAbTWIIJWRkaHmzZtr2LBhjrbIyEjZ7XbHTH7S6e9RrV+/Xn379pUkxcTEyMvLy6kmNzdXO3fudNQAAAAAQF3zdHcHKioqlJGRobFjx8rT8/91x2azKTk5WWlpaYqKilJUVJTS0tLk7++vMWPGSJKCg4OVlJSkSZMmKTQ0VCEhIUpJSVGXLl0UHx/vrlMCAAAAcIlze5Bas2aNDhw4oHvvvbfKusmTJ6ukpEQTJkxQQUGBevXqpdWrVyswMNBRM3v2bHl6eioxMVElJSWKi4tTZmamPDw86vM0AAAAAFxG3B6kBg4cKGNMtetsNptSU1OVmppa4/a+vr6aO3eu5s6de4F6CAAAAADOGsR3pAAAAADgYkKQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABY5PYg9f333+vOO+9UaGio/P391a1bN+Xk5DjWG2OUmpqq8PBw+fn5qX///tq1a5fTPkpLSzVx4kQ1bdpUAQEBGjlypA4dOlTfpwIAAADgMuHWIFVQUKB+/frJy8tL77//vr788kvNmjVLjRs3dtTMnDlT6enpmjdvnjZv3iy73a6EhAQVFxc7apKTk5WVlaXly5drw4YNOnr0qIYPH67y8nI3nBUAAACAS52nOw/+zDPPKCIiQhkZGY62Nm3aOP5sjNGcOXM0depUjR49WpK0ePFihYWFaenSpRo/frwKCwu1aNEivfLKK4qPj5ckLVmyRBEREVqzZo0GDRpU5bilpaUqLS11vC4qKrpAZwgAAADgUuTWO1Jvv/22evbsqVtuuUXNmzdX9+7d9fe//92xfu/evcrLy9PAgQMdbT4+PoqNjdXGjRslSTk5OTp58qRTTXh4uKKjox01Z5o+fbqCg4MdS0RExAU6QwAAAACXIrcGqe+++04LFixQVFSUPvjgA91///166KGH9PLLL0uS8vLyJElhYWFO24WFhTnW5eXlydvbW02aNKmx5kxTpkxRYWGhYzl48GBdnxoAAACAS5hbH+2rqKhQz549lZaWJknq3r27du3apQULFujuu+921NlsNqftjDFV2s50thofHx/5+PicZ+8BAAAAXK7cekeqRYsW6tSpk1PbVVddpQMHDkiS7Ha7JFW5s5Sfn++4S2W321VWVqaCgoIaawAAAACgLrk1SPXr10+7d+92avvmm2/UunVrSVJkZKTsdruys7Md68vKyrR+/Xr17dtXkhQTEyMvLy+nmtzcXO3cudNRAwAAAAB1ya2P9j3yyCPq27ev0tLSlJiYqM8++0wvvviiXnzxRUmnH+lLTk5WWlqaoqKiFBUVpbS0NPn7+2vMmDGSpODgYCUlJWnSpEkKDQ1VSEiIUlJS1KVLF8csfgAAAABQl9wapK655hplZWVpypQpevLJJxUZGak5c+bojjvucNRMnjxZJSUlmjBhggoKCtSrVy+tXr1agYGBjprZs2fL09NTiYmJKikpUVxcnDIzM+Xh4eGO0wIAAABwibMZY4y7O+FuRUVFCg4OVmFhoYKCgtzal61btyomJkYP/nONfnNV11pv9/1Xn2veHfHKyclRjx49LmAPAQAAgEtXbbOBW78jBQAAAAAXI4IUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALDIpSC1d+/euu4HAAAAAFw0XApSV155pQYMGKAlS5boxIkTLh88NTVVNpvNabHb7Y71xhilpqYqPDxcfn5+6t+/v3bt2uW0j9LSUk2cOFFNmzZVQECARo4cqUOHDrncJwAAAAA4F5eC1Oeff67u3btr0qRJstvtGj9+vD777DOXOtC5c2fl5uY6lh07djjWzZw5U+np6Zo3b542b94su92uhIQEFRcXO2qSk5OVlZWl5cuXa8OGDTp69KiGDx+u8vJyl/oDAAAAAOfiUpCKjo5Wenq6vv/+e2VkZCgvL0/XXXedOnfurPT0dP3000+13penp6fsdrtjadasmaTTd6PmzJmjqVOnavTo0YqOjtbixYt1/PhxLV26VJJUWFioRYsWadasWYqPj1f37t21ZMkS7dixQ2vWrHHl1AAAAADgnM5rsglPT0+NGjVKr776qp555hl9++23SklJUcuWLXX33XcrNzf3nPvYs2ePwsPDFRkZqdtuu03fffedpNPfw8rLy9PAgQMdtT4+PoqNjdXGjRslSTk5OTp58qRTTXh4uKKjox011SktLVVRUZHTAgAAAAC1dV5BasuWLZowYYJatGih9PR0paSk6Ntvv9XatWv1/fff68Ybbzzr9r169dLLL7+sDz74QH//+9+Vl5envn376siRI8rLy5MkhYWFOW0TFhbmWJeXlydvb281adKkxprqTJ8+XcHBwY4lIiLCldMHAAAAcJnydGWj9PR0ZWRkaPfu3Ro6dKhefvllDR06VI0anc5lkZGReuGFF9SxY8ez7mfIkCGOP3fp0kV9+vRRu3bttHjxYvXu3VuSZLPZnLYxxlRpO9O5aqZMmaJHH33U8bqoqIgwBQAAAKDWXLojtWDBAo0ZM0YHDhzQm2++qeHDhztCVKVWrVpp0aJFlvYbEBCgLl26aM+ePY7Z+868s5Sfn++4S2W321VWVqaCgoIaa6rj4+OjoKAgpwUAAAAAasulILVnzx5NmTLFaaryM3l7e2vs2LGW9ltaWqqvvvpKLVq0UGRkpOx2u7Kzsx3ry8rKtH79evXt21eSFBMTIy8vL6ea3Nxc7dy501EDAAAAAHXNpUf7MjIydMUVV+iWW25xan/ttdd0/PjxWgeolJQUjRgxQq1atVJ+fr6eeuopFRUVaezYsbLZbEpOTlZaWpqioqIUFRWltLQ0+fv7a8yYMZKk4OBgJSUladKkSQoNDVVISIhSUlLUpUsXxcfHu3JqAAAAAHBOLgWpGTNmaOHChVXamzdvrv/5n/+pdZA6dOiQbr/9dh0+fFjNmjVT79699emnn6p169aSpMmTJ6ukpEQTJkxQQUGBevXqpdWrVyswMNCxj9mzZ8vT01OJiYkqKSlRXFycMjMz5eHh4cqpAQAAAMA5uRSk9u/fr8jIyCrtrVu31oEDB2q9n+XLl591vc1mU2pqqlJTU2us8fX11dy5czV37txaHxcAAAAAzodL35Fq3ry5vvjiiyrtn3/+uUJDQ8+7UwAAAADQkLkUpG677TY99NBD+uijj1ReXq7y8nKtXbtWDz/8sG677ba67iMAAAAANCguPdr31FNPaf/+/YqLi5On5+ldVFRU6O6771ZaWlqddhAAAAAAGhqXgpS3t7dWrFihv/zlL/r888/l5+enLl26OCaJAAAAAIBLmUtBqlL79u3Vvn37uuoLAAAAAFwUXApS5eXlyszM1Icffqj8/HxVVFQ4rV+7dm2ddA4AAAAAGiKXgtTDDz+szMxMDRs2TNHR0bLZbHXdLwAAAABosFwKUsuXL9err76qoUOH1nV/AAAAAKDBc2n6c29vb1155ZV13RcAAAAAuCi4FKQmTZqkv/71rzLG1HV/AAAAAKDBc+nRvg0bNuijjz7S+++/r86dO8vLy8tp/cqVK+ukcwAAAADQELkUpBo3bqxRo0bVdV8AAAAA4KLgUpDKyMio634AAAAAwEXDpe9ISdKpU6e0Zs0avfDCCyouLpYk/fDDDzp69GiddQ4AAAAAGiKX7kjt379fgwcP1oEDB1RaWqqEhAQFBgZq5syZOnHihBYuXFjX/QQAAACABsOlO1IPP/ywevbsqYKCAvn5+TnaR40apQ8//LDOOgcAAAAADZHLs/b9+9//lre3t1N769at9f3339dJxwAAAACgoXLpjlRFRYXKy8urtB86dEiBgYHn3SkAAAAAaMhcClIJCQmaM2eO47XNZtPRo0f1xBNPaOjQoXXVNwAAAABokFx6tG/27NkaMGCAOnXqpBMnTmjMmDHas2ePmjZtqmXLltV1HwEAAACgQXEpSIWHh2v79u1atmyZtm7dqoqKCiUlJemOO+5wmnwCAAAAAC5FLgUpSfLz89O9996re++9ty77AwAAAAANnktB6uWXXz7r+rvvvtulzgAAAADAxcClIPXwww87vT558qSOHz8ub29v+fv7E6QAAAAAXNJcmrWvoKDAaTl69Kh2796t6667jskmAAAAAFzyXApS1YmKitKMGTOq3K0CAAAAgEtNnQUpSfLw8NAPP/xQl7sEAAAAgAbHpe9Ivf32206vjTHKzc3VvHnz1K9fvzrpGAAAAAA0VC4FqZtuusnptc1mU7NmzfTb3/5Ws2bNqot+AQAAAECD5VKQqqioqOt+AAAAAMBFo06/IwUAAAAAlwOX7kg9+uijta5NT0935RAAAAAA0GC5FKS2bdumrVu36tSpU+rQoYMk6ZtvvpGHh4d69OjhqLPZbHXTSwAAAABoQFwKUiNGjFBgYKAWL16sJk2aSDr9S3rvueceXX/99Zo0aVKddhIAAAAAGhKXviM1a9YsTZ8+3RGiJKlJkyZ66qmnmLUPAAAAwCXPpSBVVFSkH3/8sUp7fn6+iouLz7tTAAAAANCQuRSkRo0apXvuuUevv/66Dh06pEOHDun1119XUlKSRo8eXdd9BAAAAIAGxaXvSC1cuFApKSm68847dfLkydM78vRUUlKSnn322TrtIAAAAAA0NC4FKX9/f82fP1/PPvusvv32WxljdOWVVyogIKCu+wcAAAAADc55/ULe3Nxc5ebmqn379goICJAxpq76BQAAAAANlktB6siRI4qLi1P79u01dOhQ5ebmSpLuu+8+pj4HAAAAcMlzKUg98sgj8vLy0oEDB+Tv7+9ov/XWW7Vq1SqXOjJ9+nTZbDYlJyc72owxSk1NVXh4uPz8/NS/f3/t2rXLabvS0lJNnDhRTZs2VUBAgEaOHKlDhw651AcAAAAAqA2XgtTq1av1zDPPqGXLlk7tUVFR2r9/v+X9bd68WS+++KKuvvpqp/aZM2cqPT1d8+bN0+bNm2W325WQkOA0xXpycrKysrK0fPlybdiwQUePHtXw4cNVXl7uyqkBAAAAwDm5FKSOHTvmdCeq0uHDh+Xj42NpX0ePHtUdd9yhv//9706/4NcYozlz5mjq1KkaPXq0oqOjtXjxYh0/flxLly6VJBUWFmrRokWaNWuW4uPj1b17dy1ZskQ7duzQmjVrXDk1AAAAADgnl4LUDTfcoJdfftnx2mazqaKiQs8++6wGDBhgaV8PPPCAhg0bpvj4eKf2vXv3Ki8vTwMHDnS0+fj4KDY2Vhs3bpQk5eTk6OTJk0414eHhio6OdtRUp7S0VEVFRU4LAAAAANSWS9OfP/vss+rfv7+2bNmisrIyTZ48Wbt27dLPP/+sf//737Xez/Lly7V161Zt3ry5yrq8vDxJUlhYmFN7WFiY4/HBvLw8eXt7O93Jqqyp3L4606dP17Rp02rdTwAAAAD4NZfuSHXq1ElffPGFrr32WiUkJOjYsWMaPXq0tm3bpnbt2tVqHwcPHtTDDz+sJUuWyNfXt8Y6m83m9NoYU6XtTOeqmTJligoLCx3LwYMHa9VnAAAAAJBcuCNV+SjdCy+8cF53dXJycpSfn6+YmBhHW3l5uT7++GPNmzdPu3fvlnT6rlOLFi0cNfn5+Y67VHa7XWVlZSooKHC6K5Wfn6++ffvWeGwfHx/L3+UCAAAAgEqW70h5eXlp586d57wrdC5xcXHasWOHtm/f7lh69uypO+64Q9u3b1fbtm1lt9uVnZ3t2KasrEzr1693hKSYmBh5eXk51eTm5mrnzp1nDVIAAAAAcD5c+o7U3XffrUWLFmnGjBkuHzgwMFDR0dFObQEBAQoNDXW0JycnKy0tTVFRUYqKilJaWpr8/f01ZswYSVJwcLCSkpI0adIkhYaGKiQkRCkpKerSpUuVySsAAAAAoK64FKTKysr00ksvKTs7Wz179lRAQIDT+vT09Drp3OTJk1VSUqIJEyaooKBAvXr10urVqxUYGOiomT17tjw9PZWYmKiSkhLFxcUpMzNTHh4eddIHAAAAADiTpSD13XffqU2bNtq5c6d69OghSfrmm2+cas7nkb9169ZV2VdqaqpSU1Nr3MbX11dz587V3LlzXT4uAAAAAFhhKUhFRUUpNzdXH330kSTp1ltv1d/+9rcqU5QDAAAAwKXM0mQTxhin1++//76OHTtWpx0CAAAAgIbOpd8jVenMYAUAAAAAlwNLQcpms1X5DtT5ToMOAAAAABcbS9+RMsZo3Lhxjl9me+LECd1///1VZu1buXJl3fUQAAAAABoYS0Fq7NixTq/vvPPOOu0MAAAAAFwMLAWpjIyMC9UPAAAAALhonNdkEwAAAABwOSJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARW4NUgsWLNDVV1+toKAgBQUFqU+fPnr//fcd640xSk1NVXh4uPz8/NS/f3/t2rXLaR+lpaWaOHGimjZtqoCAAI0cOVKHDh2q71MBAAAAcBlxa5Bq2bKlZsyYoS1btmjLli367W9/qxtvvNERlmbOnKn09HTNmzdPmzdvlt1uV0JCgoqLix37SE5OVlZWlpYvX64NGzbo6NGjGj58uMrLy911WgAAAAAucW4NUiNGjNDQoUPVvn17tW/fXk8//bSuuOIKffrppzLGaM6cOZo6dapGjx6t6OhoLV68WMePH9fSpUslSYWFhVq0aJFmzZql+Ph4de/eXUuWLNGOHTu0Zs0ad54aAAAAgEtYg/mOVHl5uZYvX65jx46pT58+2rt3r/Ly8jRw4EBHjY+Pj2JjY7Vx40ZJUk5Ojk6ePOlUEx4erujoaEdNdUpLS1VUVOS0AAAAAEBtuT1I7dixQ1dccYV8fHx0//33KysrS506dVJeXp4kKSwszKk+LCzMsS4vL0/e3t5q0qRJjTXVmT59uoKDgx1LREREHZ8VAAAAgEuZ24NUhw4dtH37dn366af6wx/+oLFjx+rLL790rLfZbE71xpgqbWc6V82UKVNUWFjoWA4ePHh+JwEAAADgsuL2IOXt7a0rr7xSPXv21PTp09W1a1f99a9/ld1ul6Qqd5by8/Mdd6nsdrvKyspUUFBQY011fHx8HDMFVi4AAAAAUFtuD1JnMsaotLRUkZGRstvtys7OdqwrKyvT+vXr1bdvX0lSTEyMvLy8nGpyc3O1c+dORw0AAAAA1DVPdx78T3/6k4YMGaKIiAgVFxdr+fLlWrdunVatWiWbzabk5GSlpaUpKipKUVFRSktLk7+/v8aMGSNJCg4OVlJSkiZNmqTQ0FCFhIQoJSVFXbp0UXx8vDtPDQAAAMAlzK1B6scff9Rdd92l3NxcBQcH6+qrr9aqVauUkJAgSZo8ebJKSko0YcIEFRQUqFevXlq9erUCAwMd+5g9e7Y8PT2VmJiokpISxcXFKTMzUx4eHu46LQAAAACXOJsxxri7E+5WVFSk4OBgFRYWuv37Ulu3blVMTIwe/Oca/eaqrrXe7vuvPte8O+KVk5OjHj16XMAeAgAAAJeu2maDBvcdKQAAAABo6AhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWOTWIDV9+nRdc801CgwMVPPmzXXTTTdp9+7dTjXGGKWmpio8PFx+fn7q37+/du3a5VRTWlqqiRMnqmnTpgoICNDIkSN16NCh+jwVAAAAAJcRtwap9evX64EHHtCnn36q7OxsnTp1SgMHDtSxY8ccNTNnzlR6errmzZunzZs3y263KyEhQcXFxY6a5ORkZWVlafny5dqwYYOOHj2q4cOHq7y83B2nBQAAAOAS5+nOg69atcrpdUZGhpo3b66cnBzdcMMNMsZozpw5mjp1qkaPHi1JWrx4scLCwrR06VKNHz9ehYWFWrRokV555RXFx8dLkpYsWaKIiAitWbNGgwYNqvfzAgAAAHBpa1DfkSosLJQkhYSESJL27t2rvLw8DRw40FHj4+Oj2NhYbdy4UZKUk5OjkydPOtWEh4crOjraUXOm0tJSFRUVOS0AAAAAUFsNJkgZY/Too4/quuuuU3R0tCQpLy9PkhQWFuZUGxYW5liXl5cnb29vNWnSpMaaM02fPl3BwcGOJSIioq5PBwAAAMAlrMEEqQcffFBffPGFli1bVmWdzWZzem2MqdJ2prPVTJkyRYWFhY7l4MGDrnccAAAAwGWnQQSpiRMn6u2339ZHH32kli1bOtrtdrskVbmzlJ+f77hLZbfbVVZWpoKCghprzuTj46OgoCCnBQAAAABqy61ByhijBx98UCtXrtTatWsVGRnptD4yMlJ2u13Z2dmOtrKyMq1fv159+/aVJMXExMjLy8upJjc3Vzt37nTUAAAAAEBdcuusfQ888ICWLl2qt956S4GBgY47T8HBwfLz85PNZlNycrLS0tIUFRWlqKgopaWlyd/fX2PGjHHUJiUladKkSQoNDVVISIhSUlLUpUsXxyx+AAAAAFCX3BqkFixYIEnq37+/U3tGRobGjRsnSZo8ebJKSko0YcIEFRQUqFevXlq9erUCAwMd9bNnz5anp6cSExNVUlKiuLg4ZWZmysPDo75OBQAAAMBlxK1ByhhzzhqbzabU1FSlpqbWWOPr66u5c+dq7ty5ddg7AAAAAKheg5hsAgAAAAAuJgQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFbg1SH3/8sUaMGKHw8HDZbDa9+eabTuuNMUpNTVV4eLj8/PzUv39/7dq1y6mmtLRUEydOVNOmTRUQEKCRI0fq0KFD9XgWAAAAAC43bg1Sx44dU9euXTVv3rxq18+cOVPp6emaN2+eNm/eLLvdroSEBBUXFztqkpOTlZWVpeXLl2vDhg06evSohg8frvLy8vo6DQAAAACXGU93HnzIkCEaMmRIteuMMZozZ46mTp2q0aNHS5IWL16ssLAwLV26VOPHj1dhYaEWLVqkV155RfHx8ZKkJUuWKCIiQmvWrNGgQYPq7VwAAAAAXD4a7Hek9u7dq7y8PA0cONDR5uPjo9jYWG3cuFGSlJOTo5MnTzrVhIeHKzo62lFTndLSUhUVFTktAAAAAFBbDTZI5eXlSZLCwsKc2sPCwhzr8vLy5O3trSZNmtRYU53p06crODjYsURERNRx7wEAAABcyhpskKpks9mcXhtjqrSd6Vw1U6ZMUWFhoWM5ePBgnfQVAAAAwOWhwQYpu90uSVXuLOXn5zvuUtntdpWVlamgoKDGmur4+PgoKCjIaQEAAACA2mqwQSoyMlJ2u13Z2dmOtrKyMq1fv159+/aVJMXExMjLy8upJjc3Vzt37nTUAAAAAEBdc+usfUePHtV///tfx+u9e/dq+/btCgkJUatWrZScnKy0tDRFRUUpKipKaWlp8vf315gxYyRJwcHBSkpK0qRJkxQaGqqQkBClpKSoS5cujln8AAAAAKCuuTVIbdmyRQMGDHC8fvTRRyVJY8eOVWZmpiZPnqySkhJNmDBBBQUF6tWrl1avXq3AwEDHNrNnz5anp6cSExNVUlKiuLg4ZWZmysPDo97PBwAAAMDlwa1Bqn///jLG1LjeZrMpNTVVqampNdb4+vpq7ty5mjt37gXoIQAAAABU1WC/IwUAAAAADRVBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABg0SUTpObPn6/IyEj5+voqJiZGn3zyibu7BAAAAOASdUkEqRUrVig5OVlTp07Vtm3bdP3112vIkCE6cOCAu7sGAAAA4BLk6e4O1IX09HQlJSXpvvvukyTNmTNHH3zwgRYsWKDp06e7uXf166uvvrK8TWlpqXx8fOptu6ZNm6pVq1aWtwMAAEDDd+DAAR0+fNjydhfb/xEv+iBVVlamnJwcPfbYY07tAwcO1MaNG6vdprS0VKWlpY7XhYWFkqSioqIL19FaOnr0qCTp+6++UNnxY7Xebt8XWySbTXfeeaf1g9pskjH1tp2Pr69eefllhYWFWd62UaNGqqioYDu2u6DbueOYbHd5bueOY7Ld5bmdO47Jdpfndj/++KPuuvtulZ44YXlbXz8/bdm8WREREZa3rUuVmcCc4/+5F32QOnz4sMrLy6v8pzwsLEx5eXnVbjN9+nRNmzatSru7B+3Xsp56tP4O5kqIOo/tSk+cUGJiomvHBAAAwCXpREmJoqOj3d0Nh+LiYgUHB9e4/qIPUpVsNpvTa2NMlbZKU6ZM0aOP/r+gUlFRoZ9//lmhoaE1blNfioqKFBERoYMHDyooKMitfYEzxqbhYmwaLsamYWN8Gi7GpuFibBquuhobY4yKi4sVHh5+1rqLPkg1bdpUHh4eVe4+5efn1/jomI+PT5Xv9jRu3PhCddElQUFBfDgbKMam4WJsGi7GpmFjfBouxqbhYmwarroYm7Pdiap00c/a5+3trZiYGGVnZzu1Z2dnq2/fvm7qFQAAAIBL2UV/R0qSHn30Ud11113q2bOn+vTpoxdffFEHDhzQ/fff7+6uAQAAALgEXRJB6tZbb9WRI0f05JNPKjc3V9HR0frXv/6l1q1bu7trlvn4+OiJJ55waVpxXFiMTcPF2DRcjE3Dxvg0XIxNw8XYNFz1PTY2c655/QAAAAAATi7670gBAAAAQH0jSAEAAACARQQpAAAAALCIIAUAAAAAFhGk6sH8+fMVGRkpX19fxcTE6JNPPjlr/fr16xUTEyNfX1+1bdtWCxcurFLzxhtvqFOnTvLx8VGnTp2UlZV1obp/SbMyNitXrlRCQoKaNWumoKAg9enTRx988IFTTWZmpmw2W5XlxIkTF/pULjlWxmbdunXVvu9ff/21Ux2fm7phZWzGjRtX7dh07tzZUcPnpm58/PHHGjFihMLDw2Wz2fTmm2+ecxuuN/XD6thwvak/VseG6039sTo27rjeEKQusBUrVig5OVlTp07Vtm3bdP3112vIkCE6cOBAtfV79+7V0KFDdf3112vbtm3605/+pIceekhvvPGGo2bTpk269dZbddddd+nzzz/XXXfdpcTERP3nP/+pr9O6JFgdm48//lgJCQn617/+pZycHA0YMEAjRozQtm3bnOqCgoKUm5vrtPj6+tbHKV0yrI5Npd27dzu971FRUY51fG7qhtWx+etf/+o0JgcPHlRISIhuueUWpzo+N+fv2LFj6tq1q+bNm1ereq439cfq2HC9qT9Wx6YS15sLz+rYuOV6Y3BBXXvtteb+++93auvYsaN57LHHqq2fPHmy6dixo1Pb+PHjTe/evR2vExMTzeDBg51qBg0aZG677bY66vXlwerYVKdTp05m2rRpjtcZGRkmODi4rrp42bI6Nh999JGRZAoKCmrcJ5+bunG+n5usrCxjs9nMvn37HG18buqeJJOVlXXWGq437lGbsakO15sLrzZjw/XGPVz53NTH9YY7UhdQWVmZcnJyNHDgQKf2gQMHauPGjdVus2nTpir1gwYN0pYtW3Ty5Mmz1tS0T1TlyticqaKiQsXFxQoJCXFqP3r0qFq3bq2WLVtq+PDhVX6CiLM7n7Hp3r27WrRoobi4OH300UdO6/jcnL+6+NwsWrRI8fHxVX5hOp+b+sf15uLB9abh4XrT8NXH9YYgdQEdPnxY5eXlCgsLc2oPCwtTXl5etdvk5eVVW3/q1CkdPnz4rDU17RNVuTI2Z5o1a5aOHTumxMRER1vHjh2VmZmpt99+W8uWLZOvr6/69eunPXv21Gn/L2WujE2LFi304osv6o033tDKlSvVoUMHxcXF6eOPP3bU8Lk5f+f7ucnNzdX777+v++67z6mdz417cL25eHC9aTi43lwc6ut641kXncXZ2Ww2p9fGmCpt56o/s93qPlE9V9/HZcuWKTU1VW+99ZaaN2/uaO/du7d69+7teN2vXz/16NFDc+fO1d/+9re66/hlwMrYdOjQQR06dHC87tOnjw4ePKjnnntON9xwg0v7RM1cfR8zMzPVuHFj3XTTTU7tfG7ch+tNw8f1pmHhenNxqK/rDXekLqCmTZvKw8Ojyk8g8vPzq/ykopLdbq+23tPTU6GhoWetqWmfqMqVsam0YsUKJSUl6dVXX1V8fPxZaxs1aqRrrrmGnxBacD5j82u9e/d2et/53Jy/8xkbY4z+8Y9/6K677pK3t/dZa/nc1A+uNw0f15uLA9ebhqU+rzcEqQvI29tbMTExys7OdmrPzs5W3759q92mT58+VepXr16tnj17ysvL66w1Ne0TVbkyNtLpnwyOGzdOS5cu1bBhw855HGOMtm/frhYtWpx3ny8Xro7NmbZt2+b0vvO5OX/nMzbr16/Xf//7XyUlJZ3zOHxu6gfXm4aN683Fg+tNw1Kv15s6m7YC1Vq+fLnx8vIyixYtMl9++aVJTk42AQEBjhlEHnvsMXPXXXc56r/77jvj7+9vHnnkEfPll1+aRYsWGS8vL/P66687av79738bDw8PM2PGDPPVV1+ZGTNmGE9PT/Ppp5/W+/ldzKyOzdKlS42np6d5/vnnTW5urmP55ZdfHDWpqalm1apV5ttvvzXbtm0z99xzj/H09DT/+c9/6v38LmZWx2b27NkmKyvLfPPNN2bnzp3mscceM5LMG2+84ajhc1M3rI5NpTvvvNP06tWr2n3yuakbxcXFZtu2bWbbtm1GkklPTzfbtm0z+/fvN8ZwvXEnq2PD9ab+WB0brjf1x+rYVKrP6w1Bqh48//zzpnXr1sbb29v06NHDrF+/3rFu7NixJjY21ql+3bp1pnv37sbb29u0adPGLFiwoMo+X3vtNdOhQwfj5eVlOnbs6PQBRu1ZGZvY2FgjqcoyduxYR01ycrJp1aqV8fb2Ns2aNTMDBw40GzdurMczunRYGZtnnnnGtGvXzvj6+pomTZqY6667zrz33ntV9snnpm5Y/Tftl19+MX5+fubFF1+sdn98bupG5bTMNf0bxfXGfayODdeb+mN1bLje1B9X/k2r7+uNzZj//5ulAAAAAIBa4TtSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAcNH4+OOPNWLECIWHh8tms+nNN9+0vA9jjJ577jm1b99ePj4+ioiIUFpamqV9EKQAAGhAXP1PAQBcLo4dO6auXbtq3rx5Lu/j4Ycf1ksvvaTnnntOX3/9td555x1de+21lvZBkAIA1BubzXbWZdy4cee9/9qEkIYQVlJTU9WtWze39gEALkZDhgzRU089pdGjR1e7vqysTJMnT9ZvfvMbBQQEqFevXlq3bp1j/VdffaUFCxborbfe0siRIxUZGalu3bopPj7eUj88z+ckAACwIjc31/HnFStW6PHHH9fu3bsdbX5+fu7oFgDgEnLPPfdo3759Wr58ucLDw5WVlaXBgwdrx44dioqK0jvvvKO2bdvq3Xff1eDBg2WMUXx8vGbOnKmQkJBaH4c7UgCAemO32x1LcHCwbDabU9vHH3+smJgY+fr6qm3btpo2bZpOnTolSXryyScVHh6uI0eOOPY3cuRI3XDDDaqoqFCbNm0kSaNGjZLNZnO8dkVGRoauuuoq+fr6qmPHjpo/f75j3b59+2Sz2bRy5UoNGDBA/v7+6tq1qzZt2uS0j7///e+KiIiQv7+/Ro0apfT0dDVu3FiSlJmZqWnTpunzzz933I3LzMx0bHv48GGNGjVK/v7+ioqK0ttvv+3yuQDA5eTbb7/VsmXL9Nprr+n6669Xu3btlJKSouuuu04ZGRmSpO+++0779+/Xa6+9ppdfflmZmZnKycnRzTffbO1gBgAAN8jIyDDBwcGO16tWrTJBQUEmMzPTfPvtt2b16tWmTZs2JjU11RhjzKlTp0yfPn3MTTfdZIwxZsGCBSY4ONjs27fPGGNMfn6+kWQyMjJMbm6uyc/Pr/HYkkxWVla161588UXTokUL88Ybb5jvvvvOvPHGGyYkJMRkZmYaY4zZu3evkWQ6duxo3n33XbN7925z8803m9atW5uTJ08aY4zZsGGDadSokXn22WfN7t27zfPPP29CQkIc53v8+HEzadIk07lzZ5Obm2tyc3PN8ePHHX1r2bKlWbp0qdmzZ4956KGHzBVXXGGOHDni8nsNAJeqM/89f/XVV40kExAQ4LR4enqaxMREY4wxv//9740ks3v3bsd2OTk5RpL5+uuva31sHu0DADQITz/9tB577DGNHTtWktS2bVv95S9/0eTJk/XEE0/Iw8NDS5YsUbdu3fTYY49p7ty5evHFF9W6dWtJUrNmzSRJjRs3lt1ud7kff/nLXzRr1izHs/eRkZH68ssv9cILLzj6JkkpKSkaNmyYJGnatGnq3Lmz/vvf/6pjx46aO3euhgwZopSUFElS+/bttXHjRr377ruSTj/CeMUVV8jT07Pavo4bN0633367JCktLU1z587VZ599psGDB7t8XgBwOaioqJCHh4dycnLk4eHhtO6KK66QJLVo0UKenp5q3769Y91VV10lSTpw4IA6dOhQq2MRpAAADUJOTo42b96sp59+2tFWXl6uEydO6Pjx4/L391fbtm313HPPafz48br11lt1xx131GkffvrpJx08eFBJSUn6/e9/72g/deqUgoODnWqvvvpqx59btGghScrPz1fHjh21e/dujRo1yqn+2muvdQSpc/n1vgMCAhQYGKj8/HzL5wMAl5vu3burvLxc+fn5uv7666ut6devn06dOqVvv/1W7dq1kyR98803kuT44VxtEKQAAA1CRUWFpk2bVu0sTL6+vo4/f/zxx/Lw8NC+fft06tQpeXrW3aWsoqJC0unvN/Xq1ctp3Zk/2fTy8nL82WazOW1vjHG0VTr9BErt/Hrflfuv3DcAXO6OHj2q//73v47Xe/fu1fbt2xUSEqL27dvrjjvu0N13361Zs2ape/fuOnz4sNauXasuXbpo6NChio+PV48ePXTvvfdqzpw5qqio0AMPPKCEhASnu1TnwmQTAIAGoUePHtq9e7euvPLKKkujRqcvVytWrNDKlSu1bt06HTx4UH/5y1+c9uHl5aXy8nKX+xAWFqbf/OY3+u6776r0ITIystb76dixoz777DOnti1btji99vb2Pq++AsDlasuWLerevbu6d+8uSXr00UfVvXt3Pf7445JOTxh09913a9KkSerQoYNGjhyp//znP4qIiJAkNWrUSO+8846aNm2qG264QcOGDdNVV12l5cuXW+oHd6QAAA3C448/ruHDhysiIkK33HKLGjVqpC+++EI7duzQU089pUOHDukPf/iDnnnmGV133XXKzMzUsGHDNGTIEPXu3VuS1KZNG3344Yfq16+ffHx81KRJkxqPV/kTzF+78sorlZqaqoceekhBQUEaMmSISktLtWXLFhUUFOjRRx+t1blMnDhRN9xwg9LT0zVixAitXbtW77//vtNdqjZt2jj60LJlSwUGBsrHx8f6GwcAl5n+/fuf9S6/l5eXpk2bpmnTptVYEx4erjfeeOO8+sEdKQBAgzBo0CC9++67ys7O1jXXXKPevXsrPT1drVu3ljFG48aN07XXXqsHH3xQkpSQkKAHH3xQd955p44ePSpJmjVrlrKzsxUREeH4SWVNKn+C+etly5Ytuu+++/TSSy8pMzNTXbp0UWxsrDIzMy3dkerXr58WLlyo9PR0de3aVatWrdIjjzzi9Iji7373Ow0ePFgDBgxQs2bNtGzZMhfeNQCAu9iMlYe2AQCAS37/+9/r66+/1ieffOLurgAA6gCP9gEAcAE899xzSkhIUEBAgN5//30tXrzY6Rf7AgAubtyRAgDgAkhMTNS6detUXFystm3bauLEibr//vvd3S0AQB0hSAEAAACARUw2AQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALDo/wP5pSV5FXd1WQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data['content_length'] = data['cleaned_content'].apply(len)\n",
    "\n",
    "print(data['content_length'].describe())\n",
    "\n",
    "# length distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data['content_length'], bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Text Lengths')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_ids: torch.Size([856, 512])\n",
      "Shape of attention_mask: torch.Size([856, 512])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define a function to tokenize text data with truncation and padding\n",
    "def tokenize_data(texts, tokenizer, max_length=1024):\n",
    "    \"\"\"\n",
    "    Tokenize input text data using BERT tokenizer.\n",
    "    Parameters:\n",
    "        texts (list of str): List of text data to tokenize.\n",
    "        tokenizer (BertTokenizer): Pre-trained BERT tokenizer.\n",
    "        max_length (int): Maximum sequence length for padding/truncation.\n",
    "    Returns:\n",
    "        dict: Dictionary with tokenized input_ids, attention_mask.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,         # Truncate sequences to max_length\n",
    "        padding=\"max_length\",    # Pad sequences to max_length\n",
    "        max_length=max_length,   # Maximum sequence length\n",
    "        return_tensors=\"pt\"      # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "# Tokenize the text data (cleaned_content column)\n",
    "max_length = 512  # Define the fixed sequence length\n",
    "encodings = tokenize_data(data['cleaned_content'].tolist(), tokenizer, max_length=max_length)\n",
    "\n",
    "# Check the shape of tokenized inputs\n",
    "print(f\"Shape of input_ids: {encodings['input_ids'].shape}\")        # Shape of token IDs\n",
    "print(f\"Shape of attention_mask: {encodings['attention_mask'].shape}\")  # Shape of attention mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the dataset: 856\n",
      "Example of a dataset sample:\n",
      "{'input_ids': tensor([  101, 22042,  3696,  5993,  2744,  4650,  3191,  9394,  3343,  3696,\n",
      "         1999,  6279,  2591,  4070,  2180,  2102,  2695,  4070, 20786,  2442,\n",
      "         2421,  3696,  1999,  6279,  2591,  4070,  2180,  2102,  2695,  4070,\n",
      "        20786,  2442,  2421,  2064,  2102,  2767,  2208,  5045,  2067, 28997,\n",
      "         4971,  2047,  2299, 20739,  2491,  2028,  2154,  4971,  3333,  4487,\n",
      "         2015,  2650,  6136,  2402,  5003,  1051,  9541,  2226,  2226,  2226,\n",
      "         6160,  2225,  7110,  2102,  2166, 11623,  2994, 12065,  2102,  2175,\n",
      "         2078,  2707,  2514,  2066,  3190,  2562,  4172,  2072, 19808,  3501,\n",
      "         3903, 14406,  2208,  9680, 20739,  2491,  2207,  9857,  2244, 18592,\n",
      "         3128,  4431,  4971, 11155, 13552,  7867, 15969,  4679,  7867,  6884,\n",
      "         5745, 15994,  9680,  4691,  5598, 15969,  4679,  2123,  2102, 15121,\n",
      "         2051,  2175,  2852, 10993,  9096,  2123,  2102,  9680,  3198,  7743,\n",
      "         2113, 10506,  2480,  8132, 10657,  2693,  2474,  2404,  1038,  2067,\n",
      "         2208,  2036,  2507, 27863, 11245,  5833, 14068,  2666,  9033, 12439,\n",
      "         3728,  3662,  2490,  4971, 13463,  5490,  6692, 11362,  2202, 14068,\n",
      "         2067, 24497, 16021, 23091,  2559,  2066,  2388, 24316,  2075, 14068,\n",
      "        16078,  2208,  9680,  5147,  4942, 29234,  2094,  6608,  5993,  2744,\n",
      "         9394,  3343,  4374, 10373,  1057,  4882,  4638,  6745,  2739,  8224,\n",
      "         2739,  4638,  6745,  2739,  6207,  2739,  2064,  2102,  2562,  2067,\n",
      "         5685, 15628, 13552,  2182,  1056,  6392,  2099,  2544, 12486,  2211,\n",
      "         3041,  3204,  2208,  3555,  4971,  5496,  2920,  5977,  9803,  2238,\n",
      "        11912, 13742,  4971,  3202,  6380,  2035, 29107,  3508, 15870,  2713,\n",
      "         2093,  4487,  2015,  2650, 10320,  3507, 10687,  2804,  8618,  2420,\n",
      "        10052, 16021, 23091, 13552,  4952,  2208, 20739,  2491,  5147,  4942,\n",
      "        29234,  2094,  6608,  5993,  2744,  9394,  3343,  4374, 10373,  1057,\n",
      "         4882,  5147,  4942, 29234,  2094,  6608,  5993,  2744,  9394,  3343,\n",
      "         4374, 10373,  1057,  4882,  1057,  4882,  8727,  5386,  2089,  4374,\n",
      "         9430,  4957,  4031,  2326,  5147,  4942, 29234,  2094,  6608,  5993,\n",
      "         2744,  9394,  3343,  4374, 10373,  1057,  4882,  1057,  4882,  2112,\n",
      "         2112,  2572,  2098,  2401,  4024,  2177,  2572,  2098,  2401,  6113,\n",
      "         2773, 20110, 21722,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(13)}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for text classification.\n",
    "    This class combines tokenized inputs and labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with tokenized inputs and labels.\n",
    "        Parameters:\n",
    "            encodings (dict): Tokenized input data (input_ids and attention_mask).\n",
    "            labels (list or torch.Tensor): Corresponding labels for the data.\n",
    "        \"\"\"\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)  # Convert labels to tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a single sample from the dataset.\n",
    "        Parameters:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "        Returns:\n",
    "            dict: A dictionary containing input_ids, attention_mask, and labels.\n",
    "        \"\"\"\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}  # input_ids and attention_mask\n",
    "        item['labels'] = self.labels[idx]  # Add the label\n",
    "        return item\n",
    "\n",
    "# Prepare labels\n",
    "labels = data['label_encoded'].tolist()  # Extract labels as a list\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = TextClassificationDataset(encodings, labels)\n",
    "\n",
    "# Check the dataset\n",
    "print(f\"Number of samples in the dataset: {len(dataset)}\")\n",
    "print(\"Example of a dataset sample:\")\n",
    "print(dataset[0])  # Display the first sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 684\n",
      "Number of validation samples: 172\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = 0.8  # Proportion of data used for training\n",
    "train_indices, val_indices = train_test_split(\n",
    "    list(range(len(dataset))), \n",
    "    test_size=1 - train_size, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Subset the dataset for training and validation\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "# Define DataLoaders for training and validation\n",
    "batch_size = 16  # Define batch size\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check the size of each set\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_labels are 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and moved to device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Define the number of classes (unique labels in the dataset)\n",
    "num_labels = len(data['label_encoded'].unique())+1\n",
    "print(\"num_labels are\", num_labels)\n",
    "# Load pre-trained BERT model with a classification head\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",    # Pre-trained BERT model\n",
    "    num_labels=num_labels   # Number of output classes\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded and moved to device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer, loss function, and scheduler initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a123/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Define loss function (for classification tasks)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# Optionally, define a learning rate scheduler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.1)  # Decays learning rate every 2 epochs\n",
    "\n",
    "print(\"Optimizer, loss function, and scheduler initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Define the training loop\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): BERT model with classification head.\n",
    "        data_loader (DataLoader): DataLoader for training data.\n",
    "        loss_fn (function): Loss function (e.g., CrossEntropyLoss).\n",
    "        optimizer (torch.optim.Optimizer): Optimizer (e.g., AdamW).\n",
    "        device (torch.device): Device to run the training on (CPU or GPU).\n",
    "    Returns:\n",
    "        float: Average loss over the epoch.\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()  # Clear gradients from the previous step\n",
    "\n",
    "        # Move batch data to the target device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Return average loss\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "# Define the validation loop\n",
    "def eval_model(model, data_loader, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set.\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): BERT model with classification head.\n",
    "        data_loader (DataLoader): DataLoader for validation data.\n",
    "        loss_fn (function): Loss function (e.g., CrossEntropyLoss).\n",
    "        device (torch.device): Device to run the evaluation on (CPU or GPU).\n",
    "    Returns:\n",
    "        tuple: (average loss, accuracy)\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            # Move batch data to the target device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Compute average loss and accuracy\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▌   | 28/43 [07:33<04:24, 17.67s/it]"
     ]
    }
   ],
   "source": [
    "# Define the number of training epochs\n",
    "num_epochs = 10  # You can adjust based on your dataset and task\n",
    "\n",
    "# Track training progress\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_accuracy\": []\n",
    "}\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Train the model for one epoch\n",
    "    train_loss = train_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "    print(f\"Training loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss, val_accuracy = eval_model(model, val_loader, loss_fn, device)\n",
    "    print(f\"Validation loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save metrics for plotting or analysis\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_accuracy\"].append(val_accuracy)\n",
    "\n",
    "    # Update learning rate (if using a scheduler)\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "\n",
    "# Print summary of training\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Best validation accuracy: {max(history['val_accuracy']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test samples: 167\n"
     ]
    }
   ],
   "source": [
    "# Split off a test set if not already done\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "_, test_indices = train_test_split(list(range(len(dataset))), test_size=0.3, random_state=42)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given dataset.\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): Trained model to evaluate.\n",
    "        data_loader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        device (torch.device): Device to perform evaluation on.\n",
    "    Returns:\n",
    "        dict: Evaluation metrics including accuracy and classification report.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in data_loader:\n",
    "            # Move batch data to the target device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    report = classification_report(true_labels, predictions, output_dict=True)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"classification_report\": report}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7246\n",
      "\n",
      "Classification Report:\n",
      "{'1': {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0, 'support': 3},\n",
      " '10': {'f1-score': 0.5142857142857143,\n",
      "        'precision': 0.6,\n",
      "        'recall': 0.45,\n",
      "        'support': 20},\n",
      " '11': {'f1-score': 0.8043478260869565,\n",
      "        'precision': 0.6981132075471698,\n",
      "        'recall': 0.9487179487179487,\n",
      "        'support': 39},\n",
      " '12': {'f1-score': 0.7619047619047619,\n",
      "        'precision': 0.6349206349206349,\n",
      "        'recall': 0.9523809523809523,\n",
      "        'support': 42},\n",
      " '13': {'f1-score': 0.8999999999999999,\n",
      "        'precision': 0.9642857142857143,\n",
      "        'recall': 0.84375,\n",
      "        'support': 32},\n",
      " '14': {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0, 'support': 3},\n",
      " '2': {'f1-score': 0.8235294117647058,\n",
      "       'precision': 1.0,\n",
      "       'recall': 0.7,\n",
      "       'support': 10},\n",
      " '4': {'f1-score': 0.5,\n",
      "       'precision': 1.0,\n",
      "       'recall': 0.3333333333333333,\n",
      "       'support': 3},\n",
      " '5': {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0, 'support': 1},\n",
      " '8': {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0, 'support': 1},\n",
      " '9': {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0, 'support': 13},\n",
      " 'accuracy': 0.7245508982035929,\n",
      " 'macro avg': {'f1-score': 0.3912788830947399,\n",
      "               'precision': 0.4452108687957744,\n",
      "               'recall': 0.3843802031302031,\n",
      "               'support': 167},\n",
      " 'weighted avg': {'f1-score': 0.671799842040435,\n",
      "                  'precision': 0.6571869737613721,\n",
      "                  'recall': 0.7245508982035929,\n",
      "                  'support': 167}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a123/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/a123/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/a123/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "results = evaluate_model(model, test_loader, device)\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Test Accuracy: {results['accuracy']:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "from pprint import pprint\n",
    "print(\"\\nClassification Report:\")\n",
    "pprint(results['classification_report'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12    157\n",
       "11    127\n",
       "13     81\n",
       "10     66\n",
       "9      45\n",
       "2      27\n",
       "14     23\n",
       "1      10\n",
       "4       7\n",
       "6       6\n",
       "3       2\n",
       "8       2\n",
       "0       1\n",
       "7       1\n",
       "5       1\n",
       "Name: label_encoded, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label_encoded'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Downloading torchtext-0.18.0-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/a123/anaconda3/lib/python3.11/site-packages (from torchtext) (4.65.0)\n",
      "Requirement already satisfied: requests in /Users/a123/anaconda3/lib/python3.11/site-packages (from torchtext) (2.29.0)\n",
      "Collecting torch>=2.3.0 (from torchtext)\n",
      "  Downloading torch-2.5.1-cp311-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm0:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/a123/anaconda3/lib/python3.11/site-packages (from torchtext) (1.24.3)\n",
      "Requirement already satisfied: filelock in /Users/a123/anaconda3/lib/python3.11/site-packages (from torch>=2.3.0->torchtext) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/a123/anaconda3/lib/python3.11/site-packages (from torch>=2.3.0->torchtext) (4.9.0)\n",
      "Requirement already satisfied: networkx in /Users/a123/anaconda3/lib/python3.11/site-packages (from torch>=2.3.0->torchtext) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /Users/a123/anaconda3/lib/python3.11/site-packages (from torch>=2.3.0->torchtext) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/a123/anaconda3/lib/python3.11/site-packages (from torch>=2.3.0->torchtext) (2023.10.0)\n",
      "Collecting sympy==1.13.1 (from torch>=2.3.0->torchtext)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/a123/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/a123/anaconda3/lib/python3.11/site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/a123/anaconda3/lib/python3.11/site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/a123/anaconda3/lib/python3.11/site-packages (from requests->torchtext) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/a123/anaconda3/lib/python3.11/site-packages (from requests->torchtext) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/a123/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=2.3.0->torchtext) (2.1.1)\n",
      "Installing collected packages: sympy, torch, torchtext\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.11.1\n",
      "    Uninstalling sympy-1.11.1:\n",
      "      Successfully uninstalled sympy-1.11.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.0\n",
      "    Uninstalling torch-2.2.0:\n",
      "      Successfully uninstalled torch-2.2.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.17.0 requires torch==2.2.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed sympy-1.13.1 torch-2.5.1 torchtext-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/a123/anaconda3/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so, 0x0006): Symbol not found: __ZN3c104impl3cow23materialize_cow_storageERNS_11StorageImplE\n  Referenced from: <5436ECC1-6F45-386E-B542-D5F76A22B52C> /Users/a123/anaconda3/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so\n  Expected in:     <E4A087BF-2A73-36B5-9B71-18B1A776D3D4> /Users/a123/anaconda3/lib/python3.11/site-packages/torch/lib/libc10.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocab\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchtext/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m     _WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     20\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchtext/_extension.py:64\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m _init_extension()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchtext/_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m _load_lib(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibtorchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchtext/_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mload_library(path)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_ops.py:933\u001b[0m, in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    931\u001b[0m         _register_effectful_op(self, _EffectType.ORDERED)\n\u001b[1;32m    932\u001b[0m     yield\n\u001b[0;32m--> 933\u001b[0m finally:\n\u001b[1;32m    934\u001b[0m     if self in SIDE_EFFECTS:\n\u001b[1;32m    935\u001b[0m         del SIDE_EFFECTS[self]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ctypes/__init__.py:376\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m _dlopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, mode)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Users/a123/anaconda3/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so, 0x0006): Symbol not found: __ZN3c104impl3cow23materialize_cow_storageERNS_11StorageImplE\n  Referenced from: <5436ECC1-6F45-386E-B542-D5F76A22B52C> /Users/a123/anaconda3/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so\n  Expected in:     <E4A087BF-2A73-36B5-9B71-18B1A776D3D4> /Users/a123/anaconda3/lib/python3.11/site-packages/torch/lib/libc10.dylib"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Build vocabulary from training texts\n",
    "counter = Counter()\n",
    "for text in train_texts:  # `train_texts` 是训练集的文本列表\n",
    "    counter.update(tokenizer(text))\n",
    "\n",
    "# Create vocabulary with minimum frequency of 2\n",
    "vocab = Vocab(counter, specials=[\"<unk>\", \"<pad>\"], min_freq=2)\n",
    "\n",
    "# Index for padding and unknown tokens\n",
    "pad_idx = vocab[\"<pad>\"]\n",
    "unk_idx = vocab[\"<unk>\"]\n",
    "\n",
    "# Check vocabulary size and examples\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Example tokens: {vocab.itos[:10]}\")  # First 10 tokens in vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/a123/anaconda3/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so, 0x0006): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEENS6_INS2_12MemoryFormatEEE\n  Referenced from: <E56EB320-C4AC-37D0-8A24-935CA6B32161> /Users/a123/anaconda3/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so\n  Expected in:     <44DEDA27-4DE9-3D4A-8EDE-5AA72081319F> /Users/a123/anaconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorchtext\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchtext/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_torch_home\n\u001b[1;32m      5\u001b[0m _WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m _TORCHTEXT_DEPRECATION_MSG \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m/!\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorchtext is deprecated and the last released version will be 0.18 (this one). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can silence this warning by calling the following at the beginnign of your scripts: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`import torchtext; torchtext.disable_torchtext_deprecation_warning()`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdisable_torchtext_deprecation_warning\u001b[39m():\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m _WARN\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchtext/_extension.py:64\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m _init_extension()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchtext/_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m _load_lib(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibtorchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchtext/_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mload_library(path)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_ops.py:933\u001b[0m, in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    931\u001b[0m         _register_effectful_op(self, _EffectType.ORDERED)\n\u001b[1;32m    932\u001b[0m     yield\n\u001b[0;32m--> 933\u001b[0m finally:\n\u001b[1;32m    934\u001b[0m     if self in SIDE_EFFECTS:\n\u001b[1;32m    935\u001b[0m         del SIDE_EFFECTS[self]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ctypes/__init__.py:376\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m _dlopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, mode)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Users/a123/anaconda3/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so, 0x0006): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEENS6_INS2_12MemoryFormatEEE\n  Referenced from: <E56EB320-C4AC-37D0-8A24-935CA6B32161> /Users/a123/anaconda3/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so\n  Expected in:     <44DEDA27-4DE9-3D4A-8EDE-5AA72081319F> /Users/a123/anaconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"torchtext version: {torchtext.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.0.0\n",
      "  Downloading torch-2.0.0-cp311-none-macosx_11_0_arm64.whl (55.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchtext==0.15.0\n",
      "  Downloading torchtext-0.15.0-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/a123/anaconda3/lib/python3.11/site-packages (from torch==2.0.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/a123/anaconda3/lib/python3.11/site-packages (from torch==2.0.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/a123/anaconda3/lib/python3.11/site-packages (from torch==2.0.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/a123/anaconda3/lib/python3.11/site-packages (from torch==2.0.0) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /Users/a123/anaconda3/lib/python3.11/site-packages (from torch==2.0.0) (3.1.2)\n",
      "Requirement already satisfied: tqdm in /Users/a123/anaconda3/lib/python3.11/site-packages (from torchtext==0.15.0) (4.65.0)\n",
      "Requirement already satisfied: requests in /Users/a123/anaconda3/lib/python3.11/site-packages (from torchtext==0.15.0) (2.29.0)\n",
      "Requirement already satisfied: numpy in /Users/a123/anaconda3/lib/python3.11/site-packages (from torchtext==0.15.0) (1.24.3)\n",
      "Collecting torchdata==0.6.0 (from torchtext==0.15.0)\n",
      "  Downloading torchdata-0.6.0-cp311-cp311-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /Users/a123/anaconda3/lib/python3.11/site-packages (from torchdata==0.6.0->torchtext==0.15.0) (1.26.16)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/a123/anaconda3/lib/python3.11/site-packages (from jinja2->torch==2.0.0) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/a123/anaconda3/lib/python3.11/site-packages (from requests->torchtext==0.15.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/a123/anaconda3/lib/python3.11/site-packages (from requests->torchtext==0.15.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/a123/anaconda3/lib/python3.11/site-packages (from requests->torchtext==0.15.0) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/a123/anaconda3/lib/python3.11/site-packages (from sympy->torch==2.0.0) (1.2.1)\n",
      "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'torchtext' candidate (version 0.15.0 at https://files.pythonhosted.org/packages/b8/ec/3f1adc5bd9b80733ceed65383de54d6ea202653afe2361c24134f7dd8f8a/torchtext-0.15.0-cp311-cp311-macosx_11_0_arm64.whl (from https://pypi.org/simple/torchtext/) (requires-python:>=3.8))\n",
      "Reason for being yanked: Contains an incorrect dependency on torch\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: torch, torchdata, torchtext\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1\n",
      "    Uninstalling torch-2.5.1:\n",
      "      Successfully uninstalled torch-2.5.1\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.18.0\n",
      "    Uninstalling torchtext-0.18.0:\n",
      "      Successfully uninstalled torchtext-0.18.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.17.0 requires torch==2.2.0, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.0.0 torchdata-0.6.0 torchtext-0.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.0.0 torchtext==0.15.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "Solving environment: - \n",
      "Found conflicts! Looking for incompatible packages.\n",
      "This can take several minutes.  Press CTRL-C to abort.\n",
      "                                                                               failed\n",
      "\n",
      "UnsatisfiableError: The following specifications were found\n",
      "to be incompatible with the existing python installation in your environment:\n",
      "\n",
      "Specifications:\n",
      "\n",
      "  - torchtext==0.15.0 -> python[version='>=3.10,<3.11.0a0|>=3.8,<3.9.0a0|>=3.9,<3.10.0a0']\n",
      "\n",
      "Your python: python=3.11\n",
      "\n",
      "If python is on the left-most side of the chain, that's the version you've asked for.\n",
      "When python appears to the right, that indicates that the thing on the left is somehow\n",
      "not available for the python version you are constrained to. Note that conda will not\n",
      "change your python version to a different minor version unless you explicitly specify\n",
      "that.\n",
      "\n",
      "The following specifications were found to be incompatible with each other:\n",
      "\n",
      "Output in format: Requested package -> Available versions\n",
      "\n",
      "Package setuptools conflicts for:\n",
      "python=3.11 -> pip -> setuptools\n",
      "pytorch==2.0.0 -> jinja2 -> setuptools\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install pytorch==2.0.0 torchtext==0.15.0 -c pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Custom Environment",
   "language": "python",
   "name": "your_environment_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
