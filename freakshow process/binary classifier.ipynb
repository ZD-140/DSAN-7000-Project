{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"cleaned_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Category</th>\n",
       "      <th>Date</th>\n",
       "      <th>URL</th>\n",
       "      <th>Content</th>\n",
       "      <th>content_length</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>label_encoded</th>\n",
       "      <th>binary_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>music</td>\n",
       "      <td>2016-09-22</td>\n",
       "      <td>http://www.usmagazine.com/entertainment/news/t...</td>\n",
       "      <td>By clicking Sign In, you agree to our Terms an...</td>\n",
       "      <td>2509</td>\n",
       "      <td>clicking sign agree term condition read privac...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>http://www.heraldscotland.com/opinion/14759994...</td>\n",
       "      <td>\\n  IF you are watching the Corbyn-Smith show ...</td>\n",
       "      <td>6499</td>\n",
       "      <td>watching corbynsmith show pondering whether sc...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>mass media</td>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>http://www.notebookreview.com/feature/nbr-flas...</td>\n",
       "      <td>Connect with more active buying teams and shap...</td>\n",
       "      <td>3138</td>\n",
       "      <td>connect active buying team shape decision make...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sideshow</td>\n",
       "      <td>2016-09-25</td>\n",
       "      <td>https://www.loc.gov/item/fsa1997018934/PP/</td>\n",
       "      <td>Top of page \\n\\n\\n    Back to Search Results\\n...</td>\n",
       "      <td>3030</td>\n",
       "      <td>top page back search result content library co...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>music</td>\n",
       "      <td>2016-09-26</td>\n",
       "      <td>http://musicfeeds.com.au/news/parquet-courts-a...</td>\n",
       "      <td>\\n\\tBy\\n\\t\\t\\t\\n\\t\\t\\tMike Hohnen\\t\\t\\n UPDATE...</td>\n",
       "      <td>2188</td>\n",
       "      <td>mike hohnen update parquet court announced mel...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    Category        Date  \\\n",
       "0           0       music  2016-09-22   \n",
       "1           1    politics  2016-09-24   \n",
       "2           2  mass media  2016-09-24   \n",
       "3           3    sideshow  2016-09-25   \n",
       "4           4       music  2016-09-26   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  http://www.usmagazine.com/entertainment/news/t...   \n",
       "1  http://www.heraldscotland.com/opinion/14759994...   \n",
       "2  http://www.notebookreview.com/feature/nbr-flas...   \n",
       "3         https://www.loc.gov/item/fsa1997018934/PP/   \n",
       "4  http://musicfeeds.com.au/news/parquet-courts-a...   \n",
       "\n",
       "                                             Content  content_length  \\\n",
       "0  By clicking Sign In, you agree to our Terms an...            2509   \n",
       "1  \\n  IF you are watching the Corbyn-Smith show ...            6499   \n",
       "2  Connect with more active buying teams and shap...            3138   \n",
       "3  Top of page \\n\\n\\n    Back to Search Results\\n...            3030   \n",
       "4  \\n\\tBy\\n\\t\\t\\t\\n\\t\\t\\tMike Hohnen\\t\\t\\n UPDATE...            2188   \n",
       "\n",
       "                                     cleaned_content  label_encoded  \\\n",
       "0  clicking sign agree term condition read privac...              5   \n",
       "1  watching corbynsmith show pondering whether sc...              6   \n",
       "2  connect active buying team shape decision make...              4   \n",
       "3  top page back search result content library co...              7   \n",
       "4  mike hohnen update parquet court announced mel...              5   \n",
       "\n",
       "   binary_label  \n",
       "0             0  \n",
       "1             1  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    654\n",
      "1    183\n",
      "Name: binary_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data['binary_label'] = data['label_encoded'].apply(lambda x: 1 if x == 6 else 0)\n",
    "\n",
    "print(data['binary_label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'cleaned_content' miss value：0\n",
      "'binary label' miss value：0\n"
     ]
    }
   ],
   "source": [
    "missing_cleaned_content = data['cleaned_content'].isnull().sum()\n",
    "missing_label_encoded = data['binary_label'].isnull().sum()\n",
    "\n",
    "print(f\"'cleaned_content' miss value：{missing_cleaned_content}\")\n",
    "print(f\"'binary label' miss value：{missing_label_encoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "837\n"
     ]
    }
   ],
   "source": [
    "data = data.dropna(subset=['cleaned_content']).reset_index(drop=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Category</th>\n",
       "      <th>Date</th>\n",
       "      <th>URL</th>\n",
       "      <th>Content</th>\n",
       "      <th>content_length</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>label_encoded</th>\n",
       "      <th>binary_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>music</td>\n",
       "      <td>2016-09-22</td>\n",
       "      <td>http://www.usmagazine.com/entertainment/news/t...</td>\n",
       "      <td>By clicking Sign In, you agree to our Terms an...</td>\n",
       "      <td>2509</td>\n",
       "      <td>clicking sign agree term condition read privac...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>http://www.heraldscotland.com/opinion/14759994...</td>\n",
       "      <td>\\n  IF you are watching the Corbyn-Smith show ...</td>\n",
       "      <td>6499</td>\n",
       "      <td>watching corbynsmith show pondering whether sc...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>mass media</td>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>http://www.notebookreview.com/feature/nbr-flas...</td>\n",
       "      <td>Connect with more active buying teams and shap...</td>\n",
       "      <td>3138</td>\n",
       "      <td>connect active buying team shape decision make...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sideshow</td>\n",
       "      <td>2016-09-25</td>\n",
       "      <td>https://www.loc.gov/item/fsa1997018934/PP/</td>\n",
       "      <td>Top of page \\n\\n\\n    Back to Search Results\\n...</td>\n",
       "      <td>3030</td>\n",
       "      <td>top page back search result content library co...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>music</td>\n",
       "      <td>2016-09-26</td>\n",
       "      <td>http://musicfeeds.com.au/news/parquet-courts-a...</td>\n",
       "      <td>\\n\\tBy\\n\\t\\t\\t\\n\\t\\t\\tMike Hohnen\\t\\t\\n UPDATE...</td>\n",
       "      <td>2188</td>\n",
       "      <td>mike hohnen update parquet court announced mel...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    Category        Date  \\\n",
       "0           0       music  2016-09-22   \n",
       "1           1    politics  2016-09-24   \n",
       "2           2  mass media  2016-09-24   \n",
       "3           3    sideshow  2016-09-25   \n",
       "4           4       music  2016-09-26   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  http://www.usmagazine.com/entertainment/news/t...   \n",
       "1  http://www.heraldscotland.com/opinion/14759994...   \n",
       "2  http://www.notebookreview.com/feature/nbr-flas...   \n",
       "3         https://www.loc.gov/item/fsa1997018934/PP/   \n",
       "4  http://musicfeeds.com.au/news/parquet-courts-a...   \n",
       "\n",
       "                                             Content  content_length  \\\n",
       "0  By clicking Sign In, you agree to our Terms an...            2509   \n",
       "1  \\n  IF you are watching the Corbyn-Smith show ...            6499   \n",
       "2  Connect with more active buying teams and shap...            3138   \n",
       "3  Top of page \\n\\n\\n    Back to Search Results\\n...            3030   \n",
       "4  \\n\\tBy\\n\\t\\t\\t\\n\\t\\t\\tMike Hohnen\\t\\t\\n UPDATE...            2188   \n",
       "\n",
       "                                     cleaned_content  label_encoded  \\\n",
       "0  clicking sign agree term condition read privac...              5   \n",
       "1  watching corbynsmith show pondering whether sc...              6   \n",
       "2  connect active buying team shape decision make...              4   \n",
       "3  top page back search result content library co...              7   \n",
       "4  mike hohnen update parquet court announced mel...              5   \n",
       "\n",
       "   binary_label  \n",
       "0             0  \n",
       "1             1  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"cleaned_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_ids: torch.Size([837, 512])\n",
      "Shape of attention_mask: torch.Size([837, 512])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define a function to tokenize text data with truncation and padding\n",
    "def tokenize_data(texts, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Tokenize input text data using BERT tokenizer.\n",
    "    Parameters:\n",
    "        texts (list of str): List of text data to tokenize.\n",
    "        tokenizer (BertTokenizer): Pre-trained BERT tokenizer.\n",
    "        max_length (int): Maximum sequence length for padding/truncation.\n",
    "    Returns:\n",
    "        dict: Dictionary with tokenized input_ids, attention_mask.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,         # Truncate sequences to max_length\n",
    "        padding=\"max_length\",    # Pad sequences to max_length\n",
    "        max_length=max_length,   # Maximum sequence length\n",
    "        return_tensors=\"pt\"      # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "# Tokenize the text data (cleaned_content column)\n",
    "max_length = 512  # Define the fixed sequence length\n",
    "encodings = tokenize_data(data['cleaned_content'].tolist(), tokenizer, max_length=max_length)\n",
    "\n",
    "# Check the shape of tokenized inputs\n",
    "print(f\"Shape of input_ids: {encodings['input_ids'].shape}\")        # Shape of token IDs\n",
    "print(f\"Shape of attention_mask: {encodings['attention_mask'].shape}\")  # Shape of attention mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the dataset: 837\n",
      "Example of a dataset sample:\n",
      "{'input_ids': tensor([  101, 22042,  3696,  5993,  2744,  4650,  3191,  9394,  3343,  3696,\n",
      "         1999,  6279,  2591,  4070,  2180,  2102,  2695,  4070, 20786,  2442,\n",
      "         2421,  3696,  1999,  6279,  2591,  4070,  2180,  2102,  2695,  4070,\n",
      "        20786,  2442,  2421,  2064,  2102,  2767,  2208,  5045,  2067, 28997,\n",
      "         4971,  2047,  2299, 20739,  2491,  2028,  2154,  4971,  3333,  4487,\n",
      "         2015,  2650,  6136,  2402,  5003,  1051,  9541,  2226,  2226,  2226,\n",
      "         6160,  2225,  7110,  2102,  2166, 11623,  2994, 12065,  2102,  2175,\n",
      "         2078,  2707,  2514,  2066,  3190,  2562,  4172,  2072, 19808,  3501,\n",
      "         3903, 14406,  2208,  9680, 20739,  2491,  2207,  9857,  2244, 18592,\n",
      "         3128,  4431,  4971, 11155, 13552,  7867, 15969,  4679,  7867,  6884,\n",
      "         5745, 15994,  9680,  4691,  5598, 15969,  4679,  2123,  2102, 15121,\n",
      "         2051,  2175,  2852, 10993,  9096,  2123,  2102,  9680,  3198,  7743,\n",
      "         2113, 10506,  2480,  8132, 10657,  2693,  2474,  2404,  1038,  2067,\n",
      "         2208,  2036,  2507, 27863, 11245,  5833, 14068,  2666,  9033, 12439,\n",
      "         3728,  3662,  2490,  4971, 13463,  5490,  6692, 11362,  2202, 14068,\n",
      "         2067, 24497, 16021, 23091,  2559,  2066,  2388, 24316,  2075, 14068,\n",
      "        16078,  2208,  9680,  5147,  4942, 29234,  2094,  6608,  5993,  2744,\n",
      "         9394,  3343,  4374, 10373,  1057,  4882,  4638,  6745,  2739,  8224,\n",
      "         2739,  4638,  6745,  2739,  6207,  2739,  2064,  2102,  2562,  2067,\n",
      "         5685, 15628, 13552,  2182,  1056,  6392,  2099,  2544, 12486,  2211,\n",
      "         3041,  3204,  2208,  3555,  4971,  5496,  2920,  5977,  9803,  2238,\n",
      "        11912, 13742,  4971,  3202,  6380,  2035, 29107,  3508, 15870,  2713,\n",
      "         2093,  4487,  2015,  2650, 10320,  3507, 10687,  2804,  8618,  2420,\n",
      "        10052, 16021, 23091, 13552,  4952,  2208, 20739,  2491,  5147,  4942,\n",
      "        29234,  2094,  6608,  5993,  2744,  9394,  3343,  4374, 10373,  1057,\n",
      "         4882,  5147,  4942, 29234,  2094,  6608,  5993,  2744,  9394,  3343,\n",
      "         4374, 10373,  1057,  4882,  1057,  4882,  8727,  5386,  2089,  4374,\n",
      "         9430,  4957,  4031,  2326,  5147,  4942, 29234,  2094,  6608,  5993,\n",
      "         2744,  9394,  3343,  4374, 10373,  1057,  4882,  1057,  4882,  2112,\n",
      "         2112,  2572,  2098,  2401,  4024,  2177,  2572,  2098,  2401,  6113,\n",
      "         2773, 20110, 21722,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for text classification.\n",
    "    This class combines tokenized inputs and labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with tokenized inputs and labels.\n",
    "        Parameters:\n",
    "            encodings (dict): Tokenized input data (input_ids and attention_mask).\n",
    "            labels (list or torch.Tensor): Corresponding labels for the data.\n",
    "        \"\"\"\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)  # Convert labels to tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a single sample from the dataset.\n",
    "        Parameters:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "        Returns:\n",
    "            dict: A dictionary containing input_ids, attention_mask, and labels.\n",
    "        \"\"\"\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}  # input_ids and attention_mask\n",
    "        item['labels'] = self.labels[idx]  # Add the label\n",
    "        return item\n",
    "\n",
    "# Prepare labels\n",
    "labels = data['binary_label'].tolist()  # Extract labels as a list\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = TextClassificationDataset(encodings, labels)\n",
    "\n",
    "# Check the dataset\n",
    "print(f\"Number of samples in the dataset: {len(dataset)}\")\n",
    "print(\"Example of a dataset sample:\")\n",
    "print(dataset[0])  # Display the first sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 669\n",
      "Number of validation samples: 168\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "labels = data['binary_label']\n",
    "# Split dataset into training and validation sets\n",
    "train_size = 0.8  # Proportion of data used for training\n",
    "train_indices, val_indices = train_test_split(\n",
    "    list(range(len(dataset))), \n",
    "    test_size=1 - train_size,\n",
    "    stratify = labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Subset the dataset for training and validation\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "# Define DataLoaders for training and validation\n",
    "batch_size = 16  # Define batch size\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check the size of each set\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_labels are 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and moved to device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Define the number of classes (unique labels in the dataset)\n",
    "num_labels = len(data['binary_label'].unique())\n",
    "print(\"num_labels are\", num_labels)\n",
    "# Load pre-trained BERT model with a classification head\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",    # Pre-trained BERT model\n",
    "    num_labels=num_labels   # Number of output classes\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded and moved to device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer, loss function, and scheduler initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a123/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Define loss function (for classification tasks)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# Optionally, define a learning rate scheduler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.1)  # Decays learning rate every 2 epochs\n",
    "\n",
    "print(\"Optimizer, loss function, and scheduler initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Define the training loop\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): BERT model with classification head.\n",
    "        data_loader (DataLoader): DataLoader for training data.\n",
    "        loss_fn (function): Loss function (e.g., CrossEntropyLoss).\n",
    "        optimizer (torch.optim.Optimizer): Optimizer (e.g., AdamW).\n",
    "        device (torch.device): Device to run the training on (CPU or GPU).\n",
    "    Returns:\n",
    "        float: Average loss over the epoch.\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()  # Clear gradients from the previous step\n",
    "\n",
    "        # Move batch data to the target device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Return average loss\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "# Define the validation loop\n",
    "def eval_model(model, data_loader, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set.\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): BERT model with classification head.\n",
    "        data_loader (DataLoader): DataLoader for validation data.\n",
    "        loss_fn (function): Loss function (e.g., CrossEntropyLoss).\n",
    "        device (torch.device): Device to run the evaluation on (CPU or GPU).\n",
    "    Returns:\n",
    "        tuple: (average loss, accuracy)\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            # Move batch data to the target device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Compute average loss and accuracy\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/7\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 8/42 [05:36<25:21, 44.76s/it]"
     ]
    }
   ],
   "source": [
    "# Define the number of training epochs\n",
    "num_epochs = 7  # You can adjust based on your dataset and task\n",
    "\n",
    "# Track training progress\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_accuracy\": []\n",
    "}\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Train the model for one epoch\n",
    "    train_loss = train_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "    print(f\"Training loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss, val_accuracy = eval_model(model, val_loader, loss_fn, device)\n",
    "    print(f\"Validation loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save metrics for plotting or analysis\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_accuracy\"].append(val_accuracy)\n",
    "\n",
    "    # Update learning rate (if using a scheduler)\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "\n",
    "# Print summary of training\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Best validation accuracy: {max(history['val_accuracy']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Custom Environment",
   "language": "python",
   "name": "your_environment_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
